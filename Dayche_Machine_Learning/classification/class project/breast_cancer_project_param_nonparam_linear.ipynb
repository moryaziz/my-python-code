{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Classification - Practical Case\n",
    "## Initialization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Loading"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 787,
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 788,
   "outputs": [
    {
     "data": {
      "text/plain": "           id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n0      842302         M        17.99         10.38          122.80     1001.0   \n1      842517         M        20.57         17.77          132.90     1326.0   \n2    84300903         M        19.69         21.25          130.00     1203.0   \n3    84348301         M        11.42         20.38           77.58      386.1   \n4    84358402         M        20.29         14.34          135.10     1297.0   \n..        ...       ...          ...           ...             ...        ...   \n564    926424         M        21.56         22.39          142.00     1479.0   \n565    926682         M        20.13         28.25          131.20     1261.0   \n566    926954         M        16.60         28.08          108.30      858.1   \n567    927241         M        20.60         29.33          140.10     1265.0   \n568     92751         B         7.76         24.54           47.92      181.0   \n\n     smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n0            0.11840           0.27760         0.30010              0.14710   \n1            0.08474           0.07864         0.08690              0.07017   \n2            0.10960           0.15990         0.19740              0.12790   \n3            0.14250           0.28390         0.24140              0.10520   \n4            0.10030           0.13280         0.19800              0.10430   \n..               ...               ...             ...                  ...   \n564          0.11100           0.11590         0.24390              0.13890   \n565          0.09780           0.10340         0.14400              0.09791   \n566          0.08455           0.10230         0.09251              0.05302   \n567          0.11780           0.27700         0.35140              0.15200   \n568          0.05263           0.04362         0.00000              0.00000   \n\n     ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n0    ...          17.33           184.60      2019.0           0.16220   \n1    ...          23.41           158.80      1956.0           0.12380   \n2    ...          25.53           152.50      1709.0           0.14440   \n3    ...          26.50            98.87       567.7           0.20980   \n4    ...          16.67           152.20      1575.0           0.13740   \n..   ...            ...              ...         ...               ...   \n564  ...          26.40           166.10      2027.0           0.14100   \n565  ...          38.25           155.00      1731.0           0.11660   \n566  ...          34.12           126.70      1124.0           0.11390   \n567  ...          39.42           184.60      1821.0           0.16500   \n568  ...          30.37            59.16       268.6           0.08996   \n\n     compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n0              0.66560           0.7119                0.2654          0.4601   \n1              0.18660           0.2416                0.1860          0.2750   \n2              0.42450           0.4504                0.2430          0.3613   \n3              0.86630           0.6869                0.2575          0.6638   \n4              0.20500           0.4000                0.1625          0.2364   \n..                 ...              ...                   ...             ...   \n564            0.21130           0.4107                0.2216          0.2060   \n565            0.19220           0.3215                0.1628          0.2572   \n566            0.30940           0.3403                0.1418          0.2218   \n567            0.86810           0.9387                0.2650          0.4087   \n568            0.06444           0.0000                0.0000          0.2871   \n\n     fractal_dimension_worst  Unnamed: 32  \n0                    0.11890          NaN  \n1                    0.08902          NaN  \n2                    0.08758          NaN  \n3                    0.17300          NaN  \n4                    0.07678          NaN  \n..                       ...          ...  \n564                  0.07115          NaN  \n565                  0.06637          NaN  \n566                  0.07820          NaN  \n567                  0.12400          NaN  \n568                  0.07039          NaN  \n\n[569 rows x 33 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>diagnosis</th>\n      <th>radius_mean</th>\n      <th>texture_mean</th>\n      <th>perimeter_mean</th>\n      <th>area_mean</th>\n      <th>smoothness_mean</th>\n      <th>compactness_mean</th>\n      <th>concavity_mean</th>\n      <th>concave points_mean</th>\n      <th>...</th>\n      <th>texture_worst</th>\n      <th>perimeter_worst</th>\n      <th>area_worst</th>\n      <th>smoothness_worst</th>\n      <th>compactness_worst</th>\n      <th>concavity_worst</th>\n      <th>concave points_worst</th>\n      <th>symmetry_worst</th>\n      <th>fractal_dimension_worst</th>\n      <th>Unnamed: 32</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>842302</td>\n      <td>M</td>\n      <td>17.99</td>\n      <td>10.38</td>\n      <td>122.80</td>\n      <td>1001.0</td>\n      <td>0.11840</td>\n      <td>0.27760</td>\n      <td>0.30010</td>\n      <td>0.14710</td>\n      <td>...</td>\n      <td>17.33</td>\n      <td>184.60</td>\n      <td>2019.0</td>\n      <td>0.16220</td>\n      <td>0.66560</td>\n      <td>0.7119</td>\n      <td>0.2654</td>\n      <td>0.4601</td>\n      <td>0.11890</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>842517</td>\n      <td>M</td>\n      <td>20.57</td>\n      <td>17.77</td>\n      <td>132.90</td>\n      <td>1326.0</td>\n      <td>0.08474</td>\n      <td>0.07864</td>\n      <td>0.08690</td>\n      <td>0.07017</td>\n      <td>...</td>\n      <td>23.41</td>\n      <td>158.80</td>\n      <td>1956.0</td>\n      <td>0.12380</td>\n      <td>0.18660</td>\n      <td>0.2416</td>\n      <td>0.1860</td>\n      <td>0.2750</td>\n      <td>0.08902</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>84300903</td>\n      <td>M</td>\n      <td>19.69</td>\n      <td>21.25</td>\n      <td>130.00</td>\n      <td>1203.0</td>\n      <td>0.10960</td>\n      <td>0.15990</td>\n      <td>0.19740</td>\n      <td>0.12790</td>\n      <td>...</td>\n      <td>25.53</td>\n      <td>152.50</td>\n      <td>1709.0</td>\n      <td>0.14440</td>\n      <td>0.42450</td>\n      <td>0.4504</td>\n      <td>0.2430</td>\n      <td>0.3613</td>\n      <td>0.08758</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>84348301</td>\n      <td>M</td>\n      <td>11.42</td>\n      <td>20.38</td>\n      <td>77.58</td>\n      <td>386.1</td>\n      <td>0.14250</td>\n      <td>0.28390</td>\n      <td>0.24140</td>\n      <td>0.10520</td>\n      <td>...</td>\n      <td>26.50</td>\n      <td>98.87</td>\n      <td>567.7</td>\n      <td>0.20980</td>\n      <td>0.86630</td>\n      <td>0.6869</td>\n      <td>0.2575</td>\n      <td>0.6638</td>\n      <td>0.17300</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>84358402</td>\n      <td>M</td>\n      <td>20.29</td>\n      <td>14.34</td>\n      <td>135.10</td>\n      <td>1297.0</td>\n      <td>0.10030</td>\n      <td>0.13280</td>\n      <td>0.19800</td>\n      <td>0.10430</td>\n      <td>...</td>\n      <td>16.67</td>\n      <td>152.20</td>\n      <td>1575.0</td>\n      <td>0.13740</td>\n      <td>0.20500</td>\n      <td>0.4000</td>\n      <td>0.1625</td>\n      <td>0.2364</td>\n      <td>0.07678</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>564</th>\n      <td>926424</td>\n      <td>M</td>\n      <td>21.56</td>\n      <td>22.39</td>\n      <td>142.00</td>\n      <td>1479.0</td>\n      <td>0.11100</td>\n      <td>0.11590</td>\n      <td>0.24390</td>\n      <td>0.13890</td>\n      <td>...</td>\n      <td>26.40</td>\n      <td>166.10</td>\n      <td>2027.0</td>\n      <td>0.14100</td>\n      <td>0.21130</td>\n      <td>0.4107</td>\n      <td>0.2216</td>\n      <td>0.2060</td>\n      <td>0.07115</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>565</th>\n      <td>926682</td>\n      <td>M</td>\n      <td>20.13</td>\n      <td>28.25</td>\n      <td>131.20</td>\n      <td>1261.0</td>\n      <td>0.09780</td>\n      <td>0.10340</td>\n      <td>0.14400</td>\n      <td>0.09791</td>\n      <td>...</td>\n      <td>38.25</td>\n      <td>155.00</td>\n      <td>1731.0</td>\n      <td>0.11660</td>\n      <td>0.19220</td>\n      <td>0.3215</td>\n      <td>0.1628</td>\n      <td>0.2572</td>\n      <td>0.06637</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>566</th>\n      <td>926954</td>\n      <td>M</td>\n      <td>16.60</td>\n      <td>28.08</td>\n      <td>108.30</td>\n      <td>858.1</td>\n      <td>0.08455</td>\n      <td>0.10230</td>\n      <td>0.09251</td>\n      <td>0.05302</td>\n      <td>...</td>\n      <td>34.12</td>\n      <td>126.70</td>\n      <td>1124.0</td>\n      <td>0.11390</td>\n      <td>0.30940</td>\n      <td>0.3403</td>\n      <td>0.1418</td>\n      <td>0.2218</td>\n      <td>0.07820</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>567</th>\n      <td>927241</td>\n      <td>M</td>\n      <td>20.60</td>\n      <td>29.33</td>\n      <td>140.10</td>\n      <td>1265.0</td>\n      <td>0.11780</td>\n      <td>0.27700</td>\n      <td>0.35140</td>\n      <td>0.15200</td>\n      <td>...</td>\n      <td>39.42</td>\n      <td>184.60</td>\n      <td>1821.0</td>\n      <td>0.16500</td>\n      <td>0.86810</td>\n      <td>0.9387</td>\n      <td>0.2650</td>\n      <td>0.4087</td>\n      <td>0.12400</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>568</th>\n      <td>92751</td>\n      <td>B</td>\n      <td>7.76</td>\n      <td>24.54</td>\n      <td>47.92</td>\n      <td>181.0</td>\n      <td>0.05263</td>\n      <td>0.04362</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>...</td>\n      <td>30.37</td>\n      <td>59.16</td>\n      <td>268.6</td>\n      <td>0.08996</td>\n      <td>0.06444</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.2871</td>\n      <td>0.07039</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>569 rows × 33 columns</p>\n</div>"
     },
     "execution_count": 788,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# parametric models:\n",
    "## generetive models\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##  Lable Encoding and Test train splitting"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 789,
   "outputs": [
    {
     "data": {
      "text/plain": "{'divide': 'ignore', 'over': 'warn', 'under': 'ignore', 'invalid': 'ignore'}"
     },
     "execution_count": 789,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.seterr(divide='ignore', invalid='ignore')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### برای مدل های مولد باید جداگانه توزیع هر گروه فیلد هدف محاسبه شود"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "outputs": [],
   "source": [
    "data_b = pd.read_csv('data-B.csv')\n",
    "data_m = pd.read_csv('data-M.csv')\n",
    "\n",
    "X_C1 = data_b.drop(columns=['diagnosis'], inplace=False)\n",
    "X_C2 = data_m.drop(columns=['diagnosis'], inplace=False)\n",
    "encoding = LabelEncoder()\n",
    "y_C1 = encoding.fit_transform(data_b.iloc[:, 0])\n",
    "y_C2 = encoding.fit_transform(data_m.iloc[:, 0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(172, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train_C1, X_test_C1, y_train_C1, y_test_C1 = train_test_split(X_C1, y_C1, test_size=0.3)\n",
    "X_train_C2, X_test_C2, y_train_C2, y_test_C2 = train_test_split(X_C2, y_C2, test_size=0.3)\n",
    "X_test = np.vstack((X_test_C1, X_test_C2))\n",
    "\n",
    "ytest_1 = np.zeros((108, 1))\n",
    "ytest_2 = np.ones((64, 1))\n",
    "y_test = np.vstack((ytest_1, ytest_2))\n",
    "\n",
    "# y_test = np.stack((y_test_C1,y_test_C2),axis=1)\n",
    "scaler = StandardScaler().fit(X_C1)\n",
    "scaler1 = StandardScaler().fit(X_C1)\n",
    "scaler2 = StandardScaler().fit(X_C2)\n",
    "X_train_C1, X_train_C2, X_test = scaler.fit_transform(X_train_C1), scaler.fit_transform(\n",
    "    X_train_C2), scaler.fit_transform(X_test)\n",
    "print(np.shape(y_test))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### نسبت گروه های فیلد هدف در داده آموزش برای تعیین احتمال دانش پیشین"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 792,
   "outputs": [
    {
     "data": {
      "text/plain": "1.6839622641509433"
     },
     "execution_count": 792,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = X_C1.shape[0]/X_C2.shape[0]\n",
    "p"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### generative models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "outputs": [],
   "source": [
    "def mean(X):\n",
    "    return sum(X)/float(len(X))\n",
    "# m = np.mean(X) ----> numpy method\n",
    "\n",
    "def stdv(X):\n",
    "    avg = mean(X)\n",
    "    variance = []\n",
    "    for i in range(len(avg)):\n",
    "        variance.append(sum([(x - avg[i])**2 for x in X[:,i]])/float(len(X[:,i]) - 1))  # 1/(N-1))\n",
    "    return np.sqrt(np.array(variance))\n",
    "# std = np.std(X) 1/N\n",
    "\n",
    "def calculate_probability(x, mean, stdev, prior, l): # P(w!x)\n",
    "    exponent = math.exp(-((x-mean)**2 / (2 * stdev**2 )))\n",
    "    return l*prior*(1 / (math.sqrt(2 * math.pi) * stdev)) * exponent\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 794,
   "outputs": [],
   "source": [
    "m1 = np.mean(X_train_C1,axis=0)\n",
    "m2 = np.mean(X_train_C2,axis=0)\n",
    "s1 = np.std(X_train_C1,axis=0)\n",
    "s2 = np.std(X_train_C2,axis=0)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 795,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(172, 30)\n",
      "(172, 30)\n",
      "len(m1): 30\n",
      "len(X_test): 172\n",
      "[2.86446663e-01 6.61062107e-01 2.73239992e-01 3.88526581e-01\n",
      " 6.53419358e-02 2.02591085e-01 3.42598952e-01 2.88703222e-01\n",
      " 5.86078461e-01 6.57424161e-01 6.63391953e-01 1.93573997e-08\n",
      " 6.43449930e-01 6.29557209e-01 1.91594788e-01 3.95881823e-01\n",
      " 3.84465112e-01 1.19205807e-01 6.99399328e-02 4.93514347e-01\n",
      " 2.65429157e-01 5.01126333e-01 2.53638156e-01 4.02753746e-01\n",
      " 1.19325786e-02 2.44985854e-01 2.93370998e-01 1.28998843e-01\n",
      " 2.17512463e-01 3.86304775e-01]\n",
      "0.33496889948148795\n"
     ]
    }
   ],
   "source": [
    "# s1 = stdv(X_C1)\n",
    "# s2 = stdv(X_C2)\n",
    "# p = 0.5\n",
    "l1 = 1 # for risk analysis\n",
    "l2 = 1\n",
    "y_pre = []\n",
    "p1 = np.empty([X_test.shape[0], X_test.shape[1]])\n",
    "p2 = np.empty([X_test.shape[0], X_test.shape[1]])\n",
    "print(X_test.shape)\n",
    "print(p1.shape)\n",
    "print(\"len(m1):\",len(m1))\n",
    "print(\"len(X_test):\",len(X_test))\n",
    "for i in range(len(m1)):\n",
    "    for item in range(len(X_test)):\n",
    "        p1[item,i] = calculate_probability(X_test[item,i],m1[i], s1[i], p,l1)\n",
    "        p2[item,i] = calculate_probability(X_test[item,i],m2[i], s2[i], 1-p,l2)\n",
    "print(p1[0,:])\n",
    "print(np.mean(p1[0,:]))\n",
    "for item in range(len(X_test)):\n",
    "        if np.mean(p1[item,:]) > np.mean(p2[item,:]):\n",
    "            y_pre.append(0)\n",
    "        else:\n",
    "            y_pre.append(1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[108,   0],\n       [ 64,   0]], dtype=int64)"
     },
     "execution_count": 754,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_pre, y_test).T"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " ### نتیجه گیری : به نظرم میاد مدل مولد مدل خوبی نیست و همه را در گروه 0 پیش بینی کرده\n",
    "  یا کد من مشکل داره که بعدا باید بررسی شود\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Naive Bayes Classifiers\n",
    "#### همان مدل مولد است که از کد داخل پایتون استفاده شده و فرض توزیع گوسی است"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "outputs": [],
   "source": [
    "encoding = LabelEncoder()\n",
    "y = encoding.fit_transform(data.iloc[:,1])\n",
    "X = data.drop(columns=['id','diagnosis','Unnamed: 32'],inplace=False)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "outputs": [],
   "source": [
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[100,   5],\n       [  7,  59]], dtype=int64)"
     },
     "execution_count": 674,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pre = model.predict(X_test)\n",
    "confusion_matrix(y_pre, y_test).T"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# discriminative models:\n",
    "## logestic regression:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "outputs": [],
   "source": [
    "X = X.drop(columns=['diagnosis'])\n",
    "X_train , X_test , y_train , y_test = train_test_split(X,y,test_size=0.3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train, X_test = scaler.fit_transform(X_train), scaler2.fit_transform(X_test)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "y_pre = model.predict(X_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[98,  0],\n       [10, 63]], dtype=int64)"
     },
     "execution_count": 518,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_pre, y_test).T"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## parametric models:\n",
    "## risk analysis models:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### # در همه مسائل می تواند استفاده شود. اینجا ما در مثال مدل های مولد استفاده میکنیم.\n",
    "### ریسک یا همان هزینه در احتمال پیشین ضرب می شود\n",
    "#### جواب مناسبی نگرفتم چون مدل مولد در این مثال جواب مناسب نداد. شاید به خاطر تعداد بالای ویژگی باشه"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## non-parametric models:\n",
    "### Parzen density estimation\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "outputs": [
    {
     "data": {
      "text/plain": "    diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n0           B       13.540         14.36           87.46      566.3   \n1           B       13.080         15.71           85.63      520.0   \n2           B        9.504         12.44           60.34      273.9   \n3           B       13.030         18.42           82.61      523.8   \n4           B        8.196         16.84           51.71      201.9   \n..        ...          ...           ...             ...        ...   \n352         B       14.590         22.68           96.39      657.1   \n353         B       11.510         23.93           74.52      403.5   \n354         B       14.050         27.15           91.38      600.4   \n355         B       11.200         29.37           70.67      386.0   \n356         B        7.760         24.54           47.92      181.0   \n\n     smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n0            0.09779           0.08129         0.06664             0.047810   \n1            0.10750           0.12700         0.04568             0.031100   \n2            0.10240           0.06492         0.02956             0.020760   \n3            0.08983           0.03766         0.02562             0.029230   \n4            0.08600           0.05943         0.01588             0.005917   \n..               ...               ...             ...                  ...   \n352          0.08473           0.13300         0.10290             0.037360   \n353          0.09261           0.10210         0.11120             0.041050   \n354          0.09929           0.11260         0.04462             0.043040   \n355          0.07449           0.03558         0.00000             0.000000   \n356          0.05263           0.04362         0.00000             0.000000   \n\n     symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n0           0.1885  ...        15.110          19.26            99.70   \n1           0.1967  ...        14.500          20.49            96.09   \n2           0.1815  ...        10.230          15.66            65.13   \n3           0.1467  ...        13.300          22.81            84.46   \n4           0.1769  ...         8.964          21.96            57.26   \n..             ...  ...           ...            ...              ...   \n352         0.1454  ...        15.480          27.27           105.90   \n353         0.1388  ...        12.480          37.16            82.28   \n354         0.1537  ...        15.300          33.17           100.20   \n355         0.1060  ...        11.920          38.30            75.19   \n356         0.1587  ...         9.456          30.37            59.16   \n\n     area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n0         711.2           0.14400            0.17730          0.23900   \n1         630.5           0.13120            0.27760          0.18900   \n2         314.9           0.13240            0.11480          0.08867   \n3         545.9           0.09701            0.04619          0.04833   \n4         242.2           0.12970            0.13570          0.06880   \n..          ...               ...                ...              ...   \n352       733.5           0.10260            0.31710          0.36620   \n353       474.2           0.12980            0.25170          0.36300   \n354       706.7           0.12410            0.22640          0.13260   \n355       439.6           0.09267            0.05494          0.00000   \n356       268.6           0.08996            0.06444          0.00000   \n\n     concave points_worst  symmetry_worst  fractal_dimension_worst  \n0                 0.12880          0.2977                  0.07259  \n1                 0.07283          0.3184                  0.08183  \n2                 0.06227          0.2450                  0.07773  \n3                 0.05013          0.1987                  0.06169  \n4                 0.02564          0.3105                  0.07409  \n..                    ...             ...                      ...  \n352               0.11050          0.2258                  0.08004  \n353               0.09653          0.2112                  0.08732  \n354               0.10480          0.2250                  0.08321  \n355               0.00000          0.1566                  0.05905  \n356               0.00000          0.2871                  0.07039  \n\n[357 rows x 31 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>diagnosis</th>\n      <th>radius_mean</th>\n      <th>texture_mean</th>\n      <th>perimeter_mean</th>\n      <th>area_mean</th>\n      <th>smoothness_mean</th>\n      <th>compactness_mean</th>\n      <th>concavity_mean</th>\n      <th>concave points_mean</th>\n      <th>symmetry_mean</th>\n      <th>...</th>\n      <th>radius_worst</th>\n      <th>texture_worst</th>\n      <th>perimeter_worst</th>\n      <th>area_worst</th>\n      <th>smoothness_worst</th>\n      <th>compactness_worst</th>\n      <th>concavity_worst</th>\n      <th>concave points_worst</th>\n      <th>symmetry_worst</th>\n      <th>fractal_dimension_worst</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>B</td>\n      <td>13.540</td>\n      <td>14.36</td>\n      <td>87.46</td>\n      <td>566.3</td>\n      <td>0.09779</td>\n      <td>0.08129</td>\n      <td>0.06664</td>\n      <td>0.047810</td>\n      <td>0.1885</td>\n      <td>...</td>\n      <td>15.110</td>\n      <td>19.26</td>\n      <td>99.70</td>\n      <td>711.2</td>\n      <td>0.14400</td>\n      <td>0.17730</td>\n      <td>0.23900</td>\n      <td>0.12880</td>\n      <td>0.2977</td>\n      <td>0.07259</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>B</td>\n      <td>13.080</td>\n      <td>15.71</td>\n      <td>85.63</td>\n      <td>520.0</td>\n      <td>0.10750</td>\n      <td>0.12700</td>\n      <td>0.04568</td>\n      <td>0.031100</td>\n      <td>0.1967</td>\n      <td>...</td>\n      <td>14.500</td>\n      <td>20.49</td>\n      <td>96.09</td>\n      <td>630.5</td>\n      <td>0.13120</td>\n      <td>0.27760</td>\n      <td>0.18900</td>\n      <td>0.07283</td>\n      <td>0.3184</td>\n      <td>0.08183</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>B</td>\n      <td>9.504</td>\n      <td>12.44</td>\n      <td>60.34</td>\n      <td>273.9</td>\n      <td>0.10240</td>\n      <td>0.06492</td>\n      <td>0.02956</td>\n      <td>0.020760</td>\n      <td>0.1815</td>\n      <td>...</td>\n      <td>10.230</td>\n      <td>15.66</td>\n      <td>65.13</td>\n      <td>314.9</td>\n      <td>0.13240</td>\n      <td>0.11480</td>\n      <td>0.08867</td>\n      <td>0.06227</td>\n      <td>0.2450</td>\n      <td>0.07773</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>B</td>\n      <td>13.030</td>\n      <td>18.42</td>\n      <td>82.61</td>\n      <td>523.8</td>\n      <td>0.08983</td>\n      <td>0.03766</td>\n      <td>0.02562</td>\n      <td>0.029230</td>\n      <td>0.1467</td>\n      <td>...</td>\n      <td>13.300</td>\n      <td>22.81</td>\n      <td>84.46</td>\n      <td>545.9</td>\n      <td>0.09701</td>\n      <td>0.04619</td>\n      <td>0.04833</td>\n      <td>0.05013</td>\n      <td>0.1987</td>\n      <td>0.06169</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>B</td>\n      <td>8.196</td>\n      <td>16.84</td>\n      <td>51.71</td>\n      <td>201.9</td>\n      <td>0.08600</td>\n      <td>0.05943</td>\n      <td>0.01588</td>\n      <td>0.005917</td>\n      <td>0.1769</td>\n      <td>...</td>\n      <td>8.964</td>\n      <td>21.96</td>\n      <td>57.26</td>\n      <td>242.2</td>\n      <td>0.12970</td>\n      <td>0.13570</td>\n      <td>0.06880</td>\n      <td>0.02564</td>\n      <td>0.3105</td>\n      <td>0.07409</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>352</th>\n      <td>B</td>\n      <td>14.590</td>\n      <td>22.68</td>\n      <td>96.39</td>\n      <td>657.1</td>\n      <td>0.08473</td>\n      <td>0.13300</td>\n      <td>0.10290</td>\n      <td>0.037360</td>\n      <td>0.1454</td>\n      <td>...</td>\n      <td>15.480</td>\n      <td>27.27</td>\n      <td>105.90</td>\n      <td>733.5</td>\n      <td>0.10260</td>\n      <td>0.31710</td>\n      <td>0.36620</td>\n      <td>0.11050</td>\n      <td>0.2258</td>\n      <td>0.08004</td>\n    </tr>\n    <tr>\n      <th>353</th>\n      <td>B</td>\n      <td>11.510</td>\n      <td>23.93</td>\n      <td>74.52</td>\n      <td>403.5</td>\n      <td>0.09261</td>\n      <td>0.10210</td>\n      <td>0.11120</td>\n      <td>0.041050</td>\n      <td>0.1388</td>\n      <td>...</td>\n      <td>12.480</td>\n      <td>37.16</td>\n      <td>82.28</td>\n      <td>474.2</td>\n      <td>0.12980</td>\n      <td>0.25170</td>\n      <td>0.36300</td>\n      <td>0.09653</td>\n      <td>0.2112</td>\n      <td>0.08732</td>\n    </tr>\n    <tr>\n      <th>354</th>\n      <td>B</td>\n      <td>14.050</td>\n      <td>27.15</td>\n      <td>91.38</td>\n      <td>600.4</td>\n      <td>0.09929</td>\n      <td>0.11260</td>\n      <td>0.04462</td>\n      <td>0.043040</td>\n      <td>0.1537</td>\n      <td>...</td>\n      <td>15.300</td>\n      <td>33.17</td>\n      <td>100.20</td>\n      <td>706.7</td>\n      <td>0.12410</td>\n      <td>0.22640</td>\n      <td>0.13260</td>\n      <td>0.10480</td>\n      <td>0.2250</td>\n      <td>0.08321</td>\n    </tr>\n    <tr>\n      <th>355</th>\n      <td>B</td>\n      <td>11.200</td>\n      <td>29.37</td>\n      <td>70.67</td>\n      <td>386.0</td>\n      <td>0.07449</td>\n      <td>0.03558</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.1060</td>\n      <td>...</td>\n      <td>11.920</td>\n      <td>38.30</td>\n      <td>75.19</td>\n      <td>439.6</td>\n      <td>0.09267</td>\n      <td>0.05494</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.1566</td>\n      <td>0.05905</td>\n    </tr>\n    <tr>\n      <th>356</th>\n      <td>B</td>\n      <td>7.760</td>\n      <td>24.54</td>\n      <td>47.92</td>\n      <td>181.0</td>\n      <td>0.05263</td>\n      <td>0.04362</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.1587</td>\n      <td>...</td>\n      <td>9.456</td>\n      <td>30.37</td>\n      <td>59.16</td>\n      <td>268.6</td>\n      <td>0.08996</td>\n      <td>0.06444</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.2871</td>\n      <td>0.07039</td>\n    </tr>\n  </tbody>\n</table>\n<p>357 rows × 31 columns</p>\n</div>"
     },
     "execution_count": 761,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_b = pd.read_csv('data-B.csv')\n",
    "data_m = pd.read_csv('data-M.csv')\n",
    "data_b"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "outputs": [],
   "source": [
    "X_C1 = data_b.drop(columns=['diagnosis'], inplace=False)\n",
    "X_C2 = data_m.drop(columns=['diagnosis'], inplace=False)\n",
    "encoding = LabelEncoder()\n",
    "y_C1 = encoding.fit_transform(data_b.iloc[:, 0])\n",
    "y_C2 = encoding.fit_transform(data_m.iloc[:, 0])\n",
    "# X_C = np.vstack((X_C1, X_C2))\n",
    "# y_C = np.vstack((y_C1, y_C2))\n",
    "X_train_C1, X_test_C1, y_train_C1, y_test_C1 = train_test_split(X_C1, y_C1, test_size=0.3)\n",
    "X_train_C2, X_test_C2, y_train_C2, y_test_C2 = train_test_split(X_C2, y_C2, test_size=0.3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "outputs": [],
   "source": [
    "X_test = np.vstack((X_test_C1,X_test_C2))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 765,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(172, 1)\n"
     ]
    }
   ],
   "source": [
    "ytest_1 = np.zeros((108, 1))\n",
    "ytest_2 = np.ones((64, 1))\n",
    "y_test = np.vstack((ytest_1, ytest_2))\n",
    "# y_test = np.stack((y_test_C1,y_test_C2),axis=1)\n",
    "\n",
    "scaler = StandardScaler().fit(X_C1)\n",
    "scaler1 = StandardScaler().fit(X_C1)\n",
    "scaler2 = StandardScaler().fit(X_C2)\n",
    "X_train_C1, X_train_C2, X_test = scaler.fit_transform(X_train_C1), scaler.fit_transform(X_train_C2), scaler.fit_transform(X_test)\n",
    "print(np.shape(y_test))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "outputs": [],
   "source": [
    "N1 =  X_train_C1.shape[0]\n",
    "N2 =  X_train_C2.shape[0]\n",
    "pw1 = N1/(N1+N2)\n",
    "pw2 = N2/(N1+N2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "outputs": [],
   "source": [
    "class Parzen(object):\n",
    "    def __int__(self):\n",
    "        self.x_train = None\n",
    "\n",
    "    def fit(self,x_train,h):\n",
    "        self.h = h\n",
    "        self.x_train = x_train\n",
    "        self.dimension = self.x_train.shape[1]\n",
    "\n",
    "    def __kernel(self,x):\n",
    "        self.__distance = (x.reshape(1,self.dimension) - self.x_train)/self.h\n",
    "\n",
    "    def __cal_P(self,x):\n",
    "        self.__kernel(x)\n",
    "        index = np.where(np.abs(self.__distance < 0.50))\n",
    "        self.__distance[index[0]] == 1\n",
    "        self.__distance[self.__distance !=1] = 0\n",
    "        self.sum = np.sum(self.__distance)\n",
    "        return self.sum/(len(self.x_train)*(self.h**self.dimension)) # sum(k)/(N*h^D)\n",
    "\n",
    "    def predict(self,x):\n",
    "        y_pre = self.__cal_P(x)\n",
    "        return y_pre\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "outputs": [],
   "source": [
    "model1 = Parzen()\n",
    "model2 = Parzen()\n",
    "h = 0.1\n",
    "model1.fit(X_train_C1,h)\n",
    "model2.fit(X_train_C2,h)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "outputs": [],
   "source": [
    "y_pre = []\n",
    "for i in range(len(X_test)):\n",
    "    p1 = model1.predict(X_test[i, ]) # p(x!w1)\n",
    "    p2 = model2.predict(X_test[i, ]) # p(x!w2)\n",
    "    if pw1*p1 >= pw2*p2: # here P(w1) = P(w2) if P(w1) =! P(w2) then if P(w1)*p(x!w1) > P(w2)*p(x!w2)\n",
    "        y_pre.append(0)\n",
    "    else:\n",
    "        y_pre.append(1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[108,  64],\n       [  0,   0]], dtype=int64)"
     },
     "execution_count": 770,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_pre, y_test)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " ### از روش پارزن هم جواب نگرفتم. احتمالا این جدا کردن ورودی های باعث مشکل میشه و مشکل از کد منه"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## KNN:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(171,)\n"
     ]
    }
   ],
   "source": [
    "encoding = LabelEncoder()\n",
    "y = encoding.fit_transform(data.iloc[:, 1])\n",
    "X = data.drop(columns=['diagnosis','id', 'Unnamed: 32'], inplace=False)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "print(np.shape(y_test))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier(n_neighbors=10)\n",
    "model.fit(X_train, y_train)\n",
    "y_pre = model.predict(X_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[108,   7],\n       [  6,  50]], dtype=int64)"
     },
     "execution_count": 778,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_pre, y_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Linear discriminative models:\n",
    "### perceptron:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "outputs": [
    {
     "data": {
      "text/plain": "Perceptron()",
      "text/html": "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Perceptron()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Perceptron</label><div class=\"sk-toggleable__content\"><pre>Perceptron()</pre></div></div></div></div></div>"
     },
     "execution_count": 780,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Perceptron()\n",
    "model.fit(X_train,y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 781,
   "outputs": [],
   "source": [
    "y_pre = model.predict(X_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 782,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[75,  1],\n       [39, 56]], dtype=int64)"
     },
     "execution_count": 782,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_pre, y_test)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Linear Discriminant Analysis (Fisher)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 783,
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 784,
   "outputs": [
    {
     "data": {
      "text/plain": "LinearDiscriminantAnalysis()",
      "text/html": "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearDiscriminantAnalysis()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearDiscriminantAnalysis</label><div class=\"sk-toggleable__content\"><pre>LinearDiscriminantAnalysis()</pre></div></div></div></div></div>"
     },
     "execution_count": 784,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LinearDiscriminantAnalysis()\n",
    "model.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[113,   7],\n       [  1,  50]], dtype=int64)"
     },
     "execution_count": 785,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pre = model.predict(X_test)\n",
    "confusion_matrix(y_pre, y_test)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
